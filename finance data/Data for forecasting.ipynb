{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ca0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "import codecs\n",
    "# import seaborn as sns\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d665470",
   "metadata": {},
   "source": [
    "## Load financial data, add close price and volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9fc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Daily Data:\n",
    "bakka_daily = pd.read_excel(os.getcwd() + '\\\\daily\\\\Bakkafrost_04.01.2016-15.07.2022.xlsx')\n",
    "grieg_daily = pd.read_excel(os.getcwd() + '\\\\daily\\\\Grieg_04.01.2016-15.07.2022.xlsx')\n",
    "leroy_daily = pd.read_excel(os.getcwd() + '\\\\daily\\\\Leroy_04.01.2016-15.07.2022.xlsx')\n",
    "mowi_daily  = pd.read_excel(os.getcwd() + '\\\\daily\\\\MOWI_04.01.2016-15.07.2022.xlsx')\n",
    "salmar_daily = pd.read_excel(os.getcwd() + '\\\\daily\\\\SalMar_04.01.2016-15.07.2022.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa1b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(df):\n",
    "    col = [\"Exchange Date\", \"Close\", \"Volume\"]\n",
    "    df = df.loc[:,col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54649e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_together(bakka, grieg, leroy, mowi, salmar):\n",
    "    date = bakka['Exchange Date']\n",
    "    together = []\n",
    "    for i in (bakka, grieg, leroy, mowi, salmar):\n",
    "        together.append(df(i)) # a list of 5 list\n",
    "    closed_prices = pd.concat([date, together[0]['Close'], together[1]['Close'], \n",
    "                               together[2]['Close'], together[3]['Close'], together[4]['Close']], axis = 1)\n",
    "    volumes = pd.concat([date, together[0]['Volume'], together[1]['Volume'], \n",
    "                               together[2]['Volume'], together[3]['Volume'], together[4]['Volume']], axis = 1)\n",
    "    companies = (\"bakka\", \"grieg\", \"leroy\", \"mowi\", \"salmar\")\n",
    "    companies_names = [i+\"_close\" for i in companies]\n",
    "    companies_volumes = [i+\"_volume\" for i in companies]\n",
    "    cls = closed_prices['Close']\n",
    "    vlms = volumes['Volume']\n",
    "    cls.columns = companies_names\n",
    "    vlms.columns = companies_volumes\n",
    "    weight = 0.2\n",
    "    weighted = []\n",
    "    weighted_volumes = []\n",
    "    \n",
    "    for j in range(cls.shape[1]):\n",
    "        weighted.append((cls.iloc[:,j])*weight)\n",
    "        \n",
    "    for j in range(vlms.shape[1]):\n",
    "        weighted_volumes.append((vlms.iloc[:,j])*weight)\n",
    "        \n",
    "    weighted = pd.DataFrame(weighted)\n",
    "    weighted = weighted.transpose()\n",
    "    weight_names = [i+'_weight' for i in companies]\n",
    "    weighted.columns = weight_names\n",
    "    weighted['equally_weighted_index'] = weighted.sum(axis = 1)\n",
    "    \n",
    "    weighted_volumes = pd.DataFrame(weighted_volumes)\n",
    "    weighted_volumes = weighted_volumes.transpose()\n",
    "    weighted_volumes_names = [i+'_weight_volume' for i in companies]\n",
    "    weighted_volumes.columns = weighted_volumes_names\n",
    "    weighted_volumes['equally_weighted_volume'] = weighted_volumes.sum(axis = 1)\n",
    "\n",
    "    all_prices = pd.concat([date, cls, weighted, vlms, weighted_volumes], axis = 1)\n",
    "    # The index is normalized so that its logarithm equals one before the first observation (01:2016) \n",
    "    all_prices['norm_index'] =(all_prices['equally_weighted_index']/all_prices['equally_weighted_index'][0])\n",
    "    all_prices['log_ret'] = np.log(all_prices['norm_index']/all_prices['norm_index'].shift(1))\n",
    "    all_prices.fillna(0, inplace = True)\n",
    "    all_prices['react_label'] = np.where(all_prices['log_ret']>=0, 'Up', 'Down')\n",
    "    all_prices['year'] = pd.DatetimeIndex(all_prices['Exchange Date']).year\n",
    "    all_prices['month'] = pd.DatetimeIndex(all_prices['Exchange Date']).month\n",
    "    all_prices['day'] = pd.DatetimeIndex(all_prices['Exchange Date']).day\n",
    "    return (all_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758e0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_spi_volume = put_together(bakka_daily, grieg_daily, leroy_daily, mowi_daily, salmar_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af446076",
   "metadata": {},
   "source": [
    "## Deal with holidays, weekends etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4dd3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.realpath(__name__) # path\n",
    "drt = os.path.dirname(path)       # directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9c81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_import = drt + '/articles_sorted.csv'\n",
    "ph_import = drt + '/public_holidays_2016-2022.07.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20fb397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import daily SPI data:\n",
    "fin_dt = daily_spi_volume\n",
    "# Pre-processed articles:\n",
    "infos = pd.read_csv(articles_import, encoding = 'utf-8-sig', sep=';')\n",
    "# Import 'Public Holidays' data:\n",
    "ph = pd.read_csv(ph_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e87d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns we need\n",
    "cols = ['day', 'month','year','log_ret', 'react_label', 'equally_weighted_index', 'equally_weighted_volume'] \n",
    "# data frame\n",
    "fin_df = fin_dt.loc[:,cols] \n",
    "# remove, index column, word_count\n",
    "articles = infos.iloc[:,2:9]                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e19c8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640 6082\n"
     ]
    }
   ],
   "source": [
    "print(len(fin_df), len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f5b5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the packages\n",
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079de5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make the date series:\n",
    "def get_weekends(info):\n",
    "    info['date'] = pd.to_datetime({\"year\": info.year, \"month\": info.month, \"day\": info.day})\n",
    "    info['weekday'] = info['date'].dt.dayofweek\n",
    "    info['weekend']= info['weekday'] > 4\n",
    "    # Filter for ONLY the weekends:\n",
    "    info_weekends = info[info.weekend == True]\n",
    "    # Get the dates of the Weekends:\n",
    "    weekends_dates = pd.to_datetime({\"year\": info_weekends.year, \"month\": info_weekends.month, \"day\": info_weekends.day}).drop_duplicates(keep='first')\n",
    "    return (info, info.weekend.value_counts(), weekends_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581494ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_daily, trues_articles, weekends_dates_articles = get_weekends(articles) # 75 dates are weekends\n",
    "fin_df, trues_fin, weekends_dates_daily = get_weekends(fin_df)                   # No weekends\n",
    "ph, ph_trues, ph_weekends = get_weekends(ph)                                     # Get the weekends to delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d953b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Weekends from public holidays:\n",
    "ph = ph[(ph.weekend != True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ba7605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Fridays after 14:20 GMT to Mondays:\n",
    "# len(articles_daily.loc[(articles_daily['weekday'] == 4) & (articles_daily['hour'] >=14) & (articles_daily['minute'] >=20)]) # 142\n",
    "articles_daily.loc[(articles_daily['weekday'] == 4) & (articles_daily['hour'] >=14) & (articles_daily['minute'] >=20), 'impact'] = articles_daily['date'] + pd.to_timedelta(3, unit='D')\n",
    "\n",
    "# Change Weekends articles to Mondays:\n",
    "# len(articles_daily.loc[(articles_daily['weekday'] == 5) | (articles_daily['weekday'] == 6)]) # 83\n",
    "articles_daily.loc[(articles_daily['weekday'] == 5), 'impact'] = articles_daily['date'] + pd.to_timedelta(2, unit='D') # Saturday\n",
    "articles_daily.loc[(articles_daily['weekday'] == 6), 'impact'] = articles_daily['date'] + pd.to_timedelta(1, unit='D') # Sunday\n",
    "\n",
    "# Now, all articles published on Monday, Tuesday, Wednesday, and Thursday after 14:20 GMT\n",
    "# have an impact on next trading day's returns:\n",
    "articles_daily.loc[(articles_daily['hour'] >= 14) & (articles_daily['minute'] >= 20) & (articles_daily['impact'].isnull()), 'impact'] = articles_daily['date'] + pd.to_timedelta(1, unit='D')\n",
    "\n",
    "# Also update the remaining as having an impact on the same day returns:\n",
    "articles_daily.loc[(articles_daily['impact'].isnull()), 'impact'] = articles_daily['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b37ae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, lets check how many of the public holidays are in the 'impact' dates:\n",
    "articles_daily['ph'] = articles_daily.impact.isin(ph.date).astype(int)\n",
    "len(articles_daily.loc[articles_daily['ph']==1]) # 157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd0027cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the half trading public holidays\n",
    "ph_half = ph.loc[ph['trading'] == \"half\"]\n",
    "# Check if any of them are in the 'impact' column:\n",
    "articles_daily['ph_half'] = articles_daily.impact.isin(ph_half.date).astype(int)\n",
    "\n",
    "# We can say that if ph_half == 1, and the time the article was published is before 11:00 GMT then the impact is the date.\n",
    "articles_daily.loc[(articles_daily['ph_half']==1) & (articles_daily['hour'] <= 10), 'ph'] = 0\n",
    "# Delete the 'ph_half' column:\n",
    "articles_daily = articles_daily.iloc[: , :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4872f2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 articles left on public holidays\n",
      "24 articles left on public holidays\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    articles_daily['weekday_imp'] = articles_daily['impact'].dt.dayofweek #YES\n",
    "\n",
    "    # Change them to Monday:\n",
    "    articles_daily.loc[(articles_daily['ph'] == 1) & (articles_daily['weekday_imp'] == 4), 'impact'] = articles_daily['impact'] + pd.to_timedelta(3, unit='D')\n",
    "        \n",
    "    # Check again if those Mondays are also PH:\n",
    "    articles_daily['ph'] = articles_daily.impact.isin(ph.date).astype(int)\n",
    "    \n",
    "    # Change the weekdays again:\n",
    "    articles_daily['weekday_imp'] = articles_daily['impact'].dt.dayofweek\n",
    "    \n",
    "    # Change remaining dates:\n",
    "    articles_daily.loc[articles_daily['ph'] == 1, 'impact'] = articles_daily['impact'] + pd.to_timedelta(1, unit='D')\n",
    "    articles_daily['ph'] = articles_daily.impact.isin(ph.date).astype(int)\n",
    "    \n",
    "    # Check again how many there are that are Public Holidays:\n",
    "    if len(articles_daily.loc[articles_daily['ph']== 1]) == 0: # we want this to become 0\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"{len(articles_daily.loc[articles_daily['ph']== 1])} articles left on public holidays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "425815fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns that are not necessary:\n",
    "articles_daily = articles_daily.drop([\"weekday\", \"weekend\", \"ph\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3e66b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the dates and compare them with the daily SPI returns:\n",
    "daily_articles_dates = pd.DataFrame(pd.to_datetime(articles_daily['impact']).drop_duplicates(keep='first'))\n",
    "\n",
    "# change column name\n",
    "daily_articles_dates = daily_articles_dates.rename(columns={\"impact\":\"dates\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0bb3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will check which dates in the daily articles also exist in the daily returns:\n",
    "fin_df['common'] = fin_df['date'].isin(daily_articles_dates.dates).astype(int)\n",
    "\n",
    "# Drop unecessary columns:\n",
    "fin_df = fin_df.drop([\"weekday\", \"weekend\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "522edd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>log_ret</th>\n",
       "      <th>react_label</th>\n",
       "      <th>equally_weighted_index</th>\n",
       "      <th>equally_weighted_volume</th>\n",
       "      <th>date</th>\n",
       "      <th>common</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Up</td>\n",
       "      <td>117.806131</td>\n",
       "      <td>423161.300443</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.013838</td>\n",
       "      <td>Down</td>\n",
       "      <td>116.187156</td>\n",
       "      <td>556681.308522</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>Up</td>\n",
       "      <td>119.304562</td>\n",
       "      <td>463869.709177</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.019801</td>\n",
       "      <td>Down</td>\n",
       "      <td>116.965420</td>\n",
       "      <td>605717.138036</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>Down</td>\n",
       "      <td>116.945399</td>\n",
       "      <td>572520.243640</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  month  year   log_ret react_label  equally_weighted_index  \\\n",
       "0    4      1  2016  0.000000          Up              117.806131   \n",
       "1    5      1  2016 -0.013838        Down              116.187156   \n",
       "2    6      1  2016  0.026477          Up              119.304562   \n",
       "3    7      1  2016 -0.019801        Down              116.965420   \n",
       "4    8      1  2016 -0.000171        Down              116.945399   \n",
       "\n",
       "   equally_weighted_volume       date  common  \n",
       "0            423161.300443 2016-01-04       0  \n",
       "1            556681.308522 2016-01-05       0  \n",
       "2            463869.709177 2016-01-06       0  \n",
       "3            605717.138036 2016-01-07       0  \n",
       "4            572520.243640 2016-01-08       0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14a5857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save:\n",
    "fin_df.to_csv('daily/daily_common_forecasting.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e41bd",
   "metadata": {},
   "source": [
    "## Data for components: extended sentiment reversed for competitors and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4932e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "daily_topics_import = drt.replace('\\\\finance data', '') + '\\\\analysis\\\\analysis_topics' + '/daily_topics_forecasting.csv'\n",
    "# Extended sentiment\n",
    "daily_sentiment_path = drt.replace('\\\\finance data', '') + '\\\\analysis' + '/sentiment_daily_extend_comp_rev.xlsx'\n",
    "daily_sentiment = pd.read_excel(daily_sentiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98d021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import daily SPI data:\n",
    "daily_returns = fin_df\n",
    "# Topics data:\n",
    "daily_topics = pd.read_csv(daily_topics_import)\n",
    "# Rename the 'dates_day' column to 'date':\n",
    "daily_sentiment.rename(columns = {'dates_day':'date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d84e1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics['date'] = pd.to_datetime(daily_topics[['year', 'month', 'day']])\n",
    "daily_topics = daily_topics.drop(['year', 'month', 'day'], axis=1)\n",
    "col = daily_topics.pop('date')  # Pop the 'date' column\n",
    "daily_topics.insert(0, 'date', col)  # Insert 'date' column at the first position (0 index)\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29cea803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together:\n",
    "daily_topics = pd.merge(daily_returns, daily_topics, on=['date'], how='left')\n",
    "daily_topics = pd.merge(daily_topics, daily_sentiment, on=['date'], how='outer')\n",
    "daily_topics = daily_topics.drop([\"common\", \"react_label\", \"day\", \"month\", 'year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62e8ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.loc[7:,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29095351",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e006875",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.drop(['equally_weighted_index', 'equally_weighted_volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b260d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics[daily_topics['date'] <= '2022-07-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ce6bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DF:\n",
    "daily_topics.to_csv('daily/daily_topics_extend_comp_rev_forecasting.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b3b88",
   "metadata": {},
   "source": [
    "## Data for components: LM sentiment and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "418b21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths \n",
    "daily_topics_import = drt.replace('\\\\finance data', '') + '\\\\analysis\\\\analysis_topics' + '/daily_topics_forecasting.csv'\n",
    "# LM sentiment\n",
    "daily_sentiment_path = drt.replace('\\\\finance data', '') + '\\\\analysis' + '/sentiment_daily_LM.xlsx'\n",
    "daily_sentiment = pd.read_excel(daily_sentiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1af808ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import daily SPI data:\n",
    "daily_returns = fin_df\n",
    "# Topics data:\n",
    "daily_topics = pd.read_csv(daily_topics_import)\n",
    "# Rename the 'dates_day' column to 'date':\n",
    "daily_sentiment.rename(columns = {'dates_day':'date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a51df541",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics['date'] = pd.to_datetime(daily_topics[['year', 'month', 'day']])\n",
    "daily_topics = daily_topics.drop(['year', 'month', 'day'], axis=1)\n",
    "col = daily_topics.pop('date')  # Pop the 'date' column\n",
    "daily_topics.insert(0, 'date', col)  # Insert 'date' column at the first position (0 index)\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3192add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together:\n",
    "daily_topics = pd.merge(daily_returns, daily_topics, on=['date'], how='left')\n",
    "daily_topics = pd.merge(daily_topics, daily_sentiment, on=['date'], how='outer')\n",
    "daily_topics = daily_topics.drop([\"common\", \"react_label\", \"day\", \"month\", 'year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99018ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.loc[7:,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2873e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b33da8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.drop(['equally_weighted_index', 'equally_weighted_volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9c3f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics[daily_topics['date'] <= '2022-07-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9ccf7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DF:\n",
    "daily_topics.to_csv('daily/daily_topics_LM_forecasting.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59cf9a",
   "metadata": {},
   "source": [
    "## Combine all the data for forecasting into one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49cd37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "daily_sentiment_path = drt.replace('\\\\finance data', '') + '\\\\analysis' + '/sentiment_daily_LM.xlsx'\n",
    "daily_sentiment = pd.read_excel(daily_sentiment_path)\n",
    "daily_sentiment_extended_path = drt.replace('\\\\finance data', '') + '\\\\analysis' + '/sentiment_daily_extend_comp_rev.xlsx'\n",
    "daily_sentiment_extended = pd.read_excel(daily_sentiment_extended_path)\n",
    "daily_components_sent_extend_import = drt.replace('\\\\finance data', '') + '\\\\analysis\\\\VAR' + '/components_sent_extend_forecast.csv'\n",
    "daily_components_sent_import = drt.replace('\\\\finance data', '') + '\\\\analysis\\\\VAR' + '/components_sent_forecast.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e6cc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import daily SPI data:\n",
    "daily_returns = fin_df\n",
    "# Rename the 'dates_day' column to 'date':\n",
    "daily_sentiment.rename(columns = {'dates_day':'date'}, inplace = True)\n",
    "daily_sentiment_extended.rename(columns = {'dates_day':'date'}, inplace = True)\n",
    "# Components\n",
    "daily_components_sent_extend = pd.read_csv(daily_components_sent_extend_import)\n",
    "daily_components_sent = pd.read_csv(daily_components_sent_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37bad0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
    "daily_sentiment_extended['date'] = pd.to_datetime(daily_sentiment_extended['date'])\n",
    "daily_components_sent_extend['date'] = pd.to_datetime(daily_components_sent_extend['date'])\n",
    "daily_components_sent['date'] = pd.to_datetime(daily_components_sent['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3841ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together:\n",
    "daily_topics = pd.merge(daily_returns, daily_sentiment, on=['date'], how='left')\n",
    "daily_topics = pd.merge(daily_topics, daily_sentiment_extended, on=['date'], how='outer')\n",
    "daily_topics = pd.merge(daily_topics, daily_components_sent_extend, on=['date'], how='outer')\n",
    "daily_topics = pd.merge(daily_topics, daily_components_sent, on=['date'], how='outer')\n",
    "daily_topics = daily_topics.drop([\"common\", \"react_label\", \"day\", \"month\", 'year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5584ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.loc[7:,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c47277b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "155e5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_topics = daily_topics[daily_topics['date'] <= '2022-07-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88a54c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DF:\n",
    "path_to_file = os.getcwd().replace('\\\\finance data', '\\\\analysis\\\\forecasting')\n",
    "daily_topics.to_csv(path_to_file + '\\\\data_for_forecasting.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FishText",
   "language": "python",
   "name": "fishtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
