{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.realpath(__name__) # path\n",
    "drt = os.path.dirname(path)       # directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_import = drt + '/daily/SPI_daily.csv'\n",
    "articles_import = drt + '/articles_sorted.csv'\n",
    "ph_import = drt + '/public_holidays_2016-2022.07.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import daily SPI data:\n",
    "fin_dt = pd.read_csv(finance_import)\n",
    "# Pre-processed articles:\n",
    "clean_art = pd.read_csv(articles_import, encoding = 'utf-8-sig', sep=';')\n",
    "# Import 'Public Holidays' data:\n",
    "ph = pd.read_csv(ph_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['day', 'month','year','log_ret', 'react_label']     # get the columns we need\n",
    "fin_df = fin_dt.loc[:,cols]                                 # data frame\n",
    "articles = clean_art.iloc[:,2:9]                            # remove index column, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636 6082\n"
     ]
    }
   ],
   "source": [
    "print(len(fin_df), len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistent Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages\n",
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OSE trading times are: Monday through Friday from 9:00 am to 4:20 pm Central European Summer Time (GMT+02:00). The articles are published in GMT, which is 2 hrs before our time zone. The equivalent for the OSE is: Monday through Firday from 7:00 am to 2:20 pm GMT.\n",
    "\n",
    "To make the articles series consistent with the daily log returns we will group together articles that have been published on each day after 2:20 pm GMT.\n",
    "\n",
    "Those that fall on Friday, and the weekend have an impact on Monday.\n",
    "\n",
    "Next, we will check if there are ANY public holidays in the dailly log returns and if there are ANY public holidays in the articles time series. If there are public holidays that fall on a Friday and are inlcuded in the articles series we will check when is the next available return date and make the series consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the Weekend Dates for the articles AND the Monthly SPI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make the date series:\n",
    "def get_weekends(info):\n",
    "    info['date'] = pd.to_datetime({\"year\": info.year, \"month\": info.month, \"day\": info.day})\n",
    "    info['weekday'] = info['date'].dt.dayofweek\n",
    "    info['weekend'] = info['weekday'] > 4\n",
    "    # Filter for ONLY the weekends:\n",
    "    info_weekends = info[info.weekend == True]\n",
    "    # Get the dates of the Weekends:\n",
    "    weekends_dates = pd.to_datetime({\"year\": info_weekends.year, \"month\": info_weekends.month, \"day\": info_weekends.day}).drop_duplicates(keep='first')\n",
    "    return (info, info.weekend.value_counts(), weekends_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_daily, trues_articles, weekends_dates_articles = get_weekends(articles) # 75 dates are weekends\n",
    "fin_df, trues_fin, weekends_dates_daily = get_weekends(fin_df)                   # No weekends\n",
    "ph, ph_trues, ph_weekends = get_weekends(ph)                                     # Get the weekends to delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Weekends from public holidays:\n",
    "ph = ph[(ph.weekend != True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added a new column which we named \"impact\" and it represents the date on which the article's publication date and time is expected to have an impact on the daily SPI returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Fridays after 14:20 GMT to Mondays:\n",
    "# len(articles_daily.loc[(articles_daily['weekday'] == 4) & (articles_daily['hour'] >=14) & (articles_daily['minute'] >=20)]) # 142\n",
    "articles_daily.loc[(articles_daily['weekday'] == 4) & (articles_daily['hour'] >=14) & (articles_daily['minute'] >=20), 'impact'] = articles_daily['date'] + pd.to_timedelta(3, unit='D')\n",
    "\n",
    "# Change Weekends articles to Mondays:\n",
    "# len(articles_daily.loc[(articles_daily['weekday'] == 5) | (articles_daily['weekday'] == 6)]) # 83\n",
    "articles_daily.loc[(articles_daily['weekday'] == 5), 'impact'] = articles_daily['date'] + pd.to_timedelta(2, unit='D') # Saturday\n",
    "articles_daily.loc[(articles_daily['weekday'] == 6), 'impact'] = articles_daily['date'] + pd.to_timedelta(1, unit='D') # Sunday\n",
    "\n",
    "# Now, all articles published on Monday, Tuesday, Wednesday, and Thursday after 14:20 GMT\n",
    "# have an impact on next trading day's returns:\n",
    "articles_daily.loc[(articles_daily['hour'] >= 14) & (articles_daily['minute'] >= 20) & (articles_daily['impact'].isnull()), 'impact'] = articles_daily['date'] + pd.to_timedelta(1, unit='D')\n",
    "\n",
    "# Also update the remaining as having an impact on the same day returns:\n",
    "articles_daily.loc[(articles_daily['impact'].isnull()), 'impact'] = articles_daily['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeatedly execute this code until there are no more 'Public Holidays' entries in the 'impact' series. Each iteration checks for the presence of 'ph' and if found, the process is repeated. This continues until the data is completely free of any 'Public Holidays'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, lets check how many of the public holidays are in the 'impact' dates:\n",
    "articles_daily['ph'] = articles_daily.impact.isin(ph.date).astype(int)\n",
    "len(articles_daily.loc[articles_daily['ph']==1]) # 157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the half trading public hollidays\n",
    "ph_half = ph.loc[ph['trading'] == \"half\"]\n",
    "# Check if any of them are in the 'impact' column:\n",
    "articles_daily['ph_half'] = articles_daily.impact.isin(ph_half.date).astype(int)\n",
    "\n",
    "# We can say that if ph_half == 1, and the time the article was published is before 11:00 GMT then the impact is the date.\n",
    "articles_daily.loc[(articles_daily['ph_half']==1) & (articles_daily['hour'] <= 10), 'ph'] = 0\n",
    "# Delete the 'ph_half' column:\n",
    "articles_daily = articles_daily.iloc[: , :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 articles left on public holidays\n",
      "24 articles left on public holidays\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    articles_daily['weekday_imp'] = articles_daily['impact'].dt.dayofweek #YES\n",
    "\n",
    "    # Change them to Monday:\n",
    "    articles_daily.loc[(articles_daily['ph'] == 1) & (articles_daily['weekday_imp'] == 4), 'impact'] = articles_daily['impact'] + pd.to_timedelta(3, unit='D')\n",
    "        \n",
    "    # Check again if those Mondays are also PH:\n",
    "    articles_daily['ph'] = articles_daily.impact.isin(ph.date).astype(int)\n",
    "    \n",
    "    # Change the weekdays again:\n",
    "    articles_daily['weekday_imp'] = articles_daily['impact'].dt.dayofweek\n",
    "    \n",
    "    # Change remaining dates:\n",
    "    articles_daily.loc[articles_daily['ph'] == 1, 'impact'] = articles_daily['impact'] + pd.to_timedelta(1, unit='D')\n",
    "    articles_daily['ph'] = articles_daily.impact.isin(ph.date).astype(int)\n",
    "    \n",
    "    # Check again how many there are that are Public Holidays:\n",
    "    if len(articles_daily.loc[articles_daily['ph']== 1]) == 0: # we want this to become 0\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"{len(articles_daily.loc[articles_daily['ph']== 1])} articles left on public holidays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns that are not necessary:\n",
    "articles_daily = articles_daily.drop([\"weekday\", \"weekend\", \"ph\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the dates and compare them with the daily SPI returns:\n",
    "daily_articles_dates = pd.DataFrame(pd.to_datetime(articles_daily['impact']).drop_duplicates(keep='first'))\n",
    "\n",
    "# Change column name\n",
    "daily_articles_dates = daily_articles_dates.rename(columns={\"impact\":\"dates\"})\n",
    "\n",
    "# Save\n",
    "daily_articles_dates.to_csv('daily/daily_articles_dates.csv', index = False) # without duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of daily articles 1437 and the length of daily SPI returns is 1636 and the difference is 199\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of daily articles {len(daily_articles_dates)} and the length of daily SPI returns is {len(fin_df)} and the difference is {len(fin_df) - len(daily_articles_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked and the dates that are on public holiday are the half trading public hollidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any public holidays in the trading data:\n",
    "#fin_df['ph'] = fin_df.date.isin(ph.date).astype(int)\n",
    "#fin_df['ph_half'] = fin_df.date.isin(ph_half.date).astype(int)\n",
    "#fin_df = fin_df.iloc[:,:-2] # delete them, we don't need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will check which dates in the daily articles also exist in the daily returns:\n",
    "fin_df['common'] = fin_df['date'].isin(daily_articles_dates.dates).astype(int)\n",
    "# len(fin_df.loc[fin_df['common'] == 1]) # all 1437 are so all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>common</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dates, common]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we just make sure that all the dates from the daily articles are in the trading dates as well\n",
    "daily_articles_dates['common'] = daily_articles_dates['dates'].isin(fin_df.date).astype(int)\n",
    "daily_articles_dates.loc[daily_articles_dates['common']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns:\n",
    "fin_df = fin_df.drop([\"weekday\", \"weekend\"], axis = 1)\n",
    "\n",
    "# Save:\n",
    "fin_df.to_csv('daily/daily_common.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are constructing a DataFrame where the dates correspond to the dates of impact, rather than the publication dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns to tidy up the data set:\n",
    "daily_articles_impact = articles_daily.drop([\"date\", \"weekday_imp\", \"year\", \"month\", \"day\", \"minute\", \"hour\"], axis = 1)\n",
    "# Rename the column:\n",
    "daily_articles_impact = daily_articles_impact.rename(columns={\"impact\":\"dates\"})\n",
    "\n",
    "# We are getting year, month, and day for the 'dates' column:\n",
    "daily_articles_impact['year'] = daily_articles_impact['dates'].dt.year\n",
    "daily_articles_impact['month'] = daily_articles_impact['dates'].dt.month\n",
    "daily_articles_impact['day'] = daily_articles_impact['dates'].dt.day\n",
    "\n",
    "# Sort in ascending order:\n",
    "daily_articles_impact = daily_articles_impact.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "# Save:\n",
    "daily_articles_impact.to_csv('articles_daily_ts.csv', encoding = 'utf-8-sig', sep=';', index = False) # with duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FishText",
   "language": "python",
   "name": "fishtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
