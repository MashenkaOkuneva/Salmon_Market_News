{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a0760a",
   "metadata": {},
   "source": [
    "Script to read in the artile txt files and process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fff798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import codecs \n",
    "import re\n",
    "import utf8tochar\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ccd30",
   "metadata": {},
   "source": [
    "Our folder structure is something like this:\n",
    "\n",
    "```\n",
    "Salmon_Market_News\n",
    "|\n",
    "|---analysis:\n",
    "        \\read_txt_sorting_ts.ipynb\n",
    "|---scraping:\n",
    "    |---articles:\n",
    "        |---article_text\n",
    "            \\article1.txt\n",
    "            \\article2.txt\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "```\n",
    "\n",
    "Use os module to specify the current path, go up one, then down the scraping path to the `article_text` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71c16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of this notebook\n",
    "path = os.path.realpath(__name__)\n",
    "# dirname gives the folder the notebook is in, we replace analysis with scraping\n",
    "drt = os.path.dirname(path).replace('analysis', 'scraping')\n",
    "# now we can go down in the sraping branch to find the article texts\n",
    "textpath = os.path.join(drt, 'articles/article_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4a83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the files in the folder\n",
    "files = os.listdir(textpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c941a",
   "metadata": {},
   "source": [
    "Titles in certain blog posts were mistakenly split, with parts ending up in the article body due to the presence of '--' characters. Here we fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852cc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CSV file containing titles needing correction\n",
    "corrected_titles = pd.read_csv('corrected_titles.csv', encoding = 'utf-8', header = None)\n",
    "# extract the day from the date string\n",
    "day_correct = [date.split(' ')[0].split('-')[2] for date in corrected_titles[0][:-2]]\n",
    "# extract the month from the date string\n",
    "month_correct = [date.split(' ')[0].split('-')[1] for date in corrected_titles[0][:-2]]\n",
    "# extract the year from the date string\n",
    "year_correct = [date.split(' ')[0].split('-')[0] for date in corrected_titles[0][:-2]]\n",
    "# extract the first part of the title before '--'\n",
    "first_part = [t.split('--')[0] for t in corrected_titles[1][:-2]]\n",
    "# join all parts after the first '--'\n",
    "second_part = [''.join(t.split('--')[1:]) for t in corrected_titles[1][:-2]]\n",
    "# extract all titles\n",
    "correct_title = [t for t in corrected_titles[1][:-2]]\n",
    "# combine all the extracted information into a list of tuples\n",
    "to_correct = list(zip(day_correct, month_correct, year_correct, first_part, second_part, correct_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e4119",
   "metadata": {},
   "source": [
    "Each text file is parsed to extract and format key elements such as the publication date, title, and article body. Titles from the blogs that were incorrectly formatted are corrected here. We also remove publication dates from all blog articles. The text of each article is compiled from the title, lead, and body. Text cleaning involves removing multiple spaces, non-breaking spaces, backslashes, tabs, and carriage returns. Additionally, we correct unicode errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac60ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.66 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "day = []\n",
    "hour = []\n",
    "minute = []\n",
    "month = []\n",
    "year = []\n",
    "titles = []\n",
    "texts = []\n",
    "\n",
    "# list of time zones\n",
    "tz = ['PST', 'EST', 'GMT', 'CST', 'ICT', 'CET', 'CT']\n",
    "ampm = ['am', 'pm', 'a\\.m\\.', 'p\\.m\\.']\n",
    "char_remove = [\"\\t\", \"\\r\", \"\\\\\"]\n",
    "\n",
    "for f in files: \n",
    "    with codecs.open(os.path.join(textpath, f), \"r\", encoding = 'utf-8' ) as f:\n",
    "        f_data = f.read().split('\\n')\n",
    "        # extract the date, strip leading and trailing whitespaces\n",
    "        date = ':'.join(f_data[0].split(':')[1:]).strip()\n",
    "        # day of the publication\n",
    "        day.append(date.split()[0].split('-')[2])\n",
    "        # month of the publication\n",
    "        month.append(date.split()[0].split('-')[1])\n",
    "        # year of the publication\n",
    "        year.append(date.split()[0].split('-')[0])\n",
    "        # hour of publication\n",
    "        hour.append(date.split()[1].split(':')[0])\n",
    "        # minute of publication\n",
    "        minute.append(date.split()[1].split(':')[1])\n",
    "        # extract the title\n",
    "        title = ':'.join(f_data[1].split(':')[1:]).strip()\n",
    "        # remove tab and carriage return\n",
    "        title = title.replace(\"\\t\", '').replace(\"\\r\", '')\n",
    "        for correct in to_correct:\n",
    "            # check if the current title and date match the erroneous title and its respective date parts \n",
    "            # from the 'to_correct' list\n",
    "            if (title == correct[3].strip()) and (date.split()[0].split('-')[2] == correct[0]) and \\\n",
    "            (date.split()[0].split('-')[1] == correct[1]) and (date.split()[0].split('-')[0] == correct[2]):\n",
    "                # if they match, replace the current title with the fully corrected title\n",
    "                title = correct[5]\n",
    "        # if the title is not an empty string and there is no period, colon, semicolon, exclamation, or question mark\n",
    "        # at the end of the sentence, add the period.\n",
    "        if title != '':\n",
    "            if title[-1] not in ['.', '!', ':', ';', '?']:\n",
    "                title = title + '.'\n",
    "        titles.append(title)\n",
    "        # extract the lead\n",
    "        lead = ':'.join(f_data[3].split(':')[1:]).strip()\n",
    "        # extract the body\n",
    "        body = ':'.join(f_data[4].split(':')[1:]).strip()\n",
    "        for correct in to_correct:\n",
    "            # check if the article body starts with the misformatted part of the title (correct[4])\n",
    "            # and if the date of the article matches the date components (year, month, day) from the tuple\n",
    "            if (body.startswith(correct[4].strip())) and (date.split()[0].split('-')[2] == correct[0]) and \\\n",
    "            (date.split()[0].split('-')[1] == correct[1]) and (date.split()[0].split('-')[0] == correct[2]):\n",
    "                # if the conditions are met, replace the misformatted part in the body with an empty string,\n",
    "                # effectively removing it\n",
    "                body = body.replace(correct[4].strip(), '')\n",
    "        # remove the date from the separated blog article\n",
    "        if title == '':\n",
    "            if len(re.findall(r\".*(?:\" + '|'.join(tz) + r\")\", body)) > 0:\n",
    "                date_remove = re.findall(r\".*(?:\" + '|'.join(tz) + r\")\", body)[0]\n",
    "            else:\n",
    "                date_remove = re.findall(r\".*(?:\" + '|'.join(ampm) + r\")\", body)[0]\n",
    "            body = body.split(date_remove, 1)[1].strip()            \n",
    "        # combine title, lead, and body into a text\n",
    "        if re.sub(r'\\s{2,}', ' ', lead) not in re.sub(r'\\s{2,}', ' ', body):\n",
    "            text = title + ' ' + lead + ' ' + body\n",
    "        else:\n",
    "            text = title + ' ' + body\n",
    "        # strip whitespace on both sides\n",
    "        text = text.strip()\n",
    "        # simple text cleaning\n",
    "        # remove tab, carriage return, and backslashes\n",
    "        for ch in char_remove:\n",
    "            text = text.replace(ch, \"\")\n",
    "        # correct some unicode errors\n",
    "        for k,v in utf8tochar.utf8tochar_no_backslash.items():\n",
    "            text = text.replace(k,v)\n",
    "        # correct some unicode errors\n",
    "        for k,v in utf8tochar.encod_mistake_tochar.items():\n",
    "            text = text.replace(k,v) \n",
    "        # remove multiple spaces: {2,} means at least 2 repeats\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)\n",
    "        # replace non-breaking spaces with a space\n",
    "        text = re.sub(r'\\xa0', ' ', text)\n",
    "        # strip whitespace on both sides\n",
    "        text = text.strip()\n",
    "        texts.append(text)\n",
    "        \n",
    "# convert string to integer\n",
    "day = list(map(int, day))\n",
    "month = list(map(int, month))\n",
    "year = list(map(int, year))      \n",
    "hour = list(map(int, hour))\n",
    "minute = list(map(int, minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2170bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will create a DataFrame \n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\n",
    "    'day': day,\n",
    "    'month': month,\n",
    "    'year': year,\n",
    "    'minute': minute,\n",
    "    'hour': hour,\n",
    "    'titles': titles,\n",
    "    'texts' :texts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77a9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6183\n"
     ]
    }
   ],
   "source": [
    "# remove articles published before 2016\n",
    "data = data[data.year>=2016]\n",
    "# remove articles with no text\n",
    "data = data[data.texts != '']\n",
    "# sort the articles by year, month, day, hour, minute\n",
    "data = data.sort_values(['year', 'month', 'day', 'hour', 'minute'], ascending=[True, True, True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd34bf5",
   "metadata": {},
   "source": [
    "Articles that contain the string '...(read more )' are introductions only, they do not contain the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daba54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[(data['texts'].str.contains('''...(read more )''', na = False, regex = False))].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a423f4",
   "metadata": {},
   "source": [
    "In some cases, a period and the word following it are mistakenly merged together. To ensure that all texts have the expected format, we correct this mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee47b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.913444\n"
     ]
    }
   ],
   "source": [
    "# Set the number of cores to use\n",
    "NUM_CORE = mp.cpu_count()-4\n",
    "startTime = datetime.now()\n",
    "\n",
    "import split_period_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    texts_corrected = pool.map(split_period_word.split_period_word, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
    "data['texts'] = texts_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e36afe",
   "metadata": {},
   "source": [
    "Similar to stopwords, there are strings that do not help us determine sentiment or topic of the text. Therefore, we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3cf597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:10.398403\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import clean_text\n",
    "\n",
    "data = clean_text.delete_articles_with_strings(clean_text.delete_article_strings, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    cleaned_articles = pool.map(clean_text.clean_text, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "data['texts'] = cleaned_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710cf4a2",
   "metadata": {},
   "source": [
    "Identify name candidates using spacy. We use these candidates to find the names of IntraFish journalists that we later delete.\n",
    "\n",
    "To install spaCy and a small English pipeline via conda:\n",
    "\n",
    "`conda install -c conda-forge spacy`\n",
    "\n",
    "`python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0023f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:37.529003\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import identify_names\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    names = pool.map(identify_names.identify_names, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367e2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [item for sublist in names for item in sublist]\n",
    "# List of unique names\n",
    "names = list(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02a90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_freq = dict()\n",
    "for name in names:\n",
    "    if '(' in name:\n",
    "        name = name.split('(')[0]\n",
    "    elif ')' in name:\n",
    "        name = name.split(')')[0]\n",
    "    name_freq[name] = len(data[data.texts.str.contains(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0343a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted(name_freq.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed2c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hvistendahl', 9), ('Blumar Seafoods', 9), ('Fellow', 9), ('Nilsen', 9), ('Bradley', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_dict[900:905])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec6a63",
   "metadata": {},
   "source": [
    "Correct some misspellings produced by the funciton `split_period_word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d35b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = []\n",
    "for text in data.texts:\n",
    "    # Inc. not\n",
    "    text = re.sub('\\. not(?!e)', '.not', text)     \n",
    "    # DNBNXT. no\n",
    "    text = re.sub('\\. no(?!w|te)', '.no', text)\n",
    "    # JD. com\n",
    "    text = re.sub('\\. com', '.com', text) \n",
    "    # Portalspozywczy. pl\n",
    "    text = re.sub('\\. pl(?!ay|astic)', '.pl', text)\n",
    "    # insider. co.uk\n",
    "    text = re.sub('\\. co(?!nventional|nsumers)', '.co', text)\n",
    "    # Mathem. se\n",
    "    text = re.sub('\\. se(?!afood)', '.se', text)   \n",
    "    # Kuow. org\n",
    "    text = re.sub('\\. org(?!afood)', '.org', text)\n",
    "    # abc. net\n",
    "    text = re.sub('\\. net(?!afood)', '.net', text)\n",
    "    # biobiochile. cl\n",
    "    text = re.sub('\\. cl(?!ams)', '.cl', text)  \n",
    "    # C. opilio\n",
    "    text = re.sub('\\. opilio', '.opilio', text)\n",
    "    # oursafety. info\n",
    "    text = re.sub('\\. info', '.info', text)\n",
    "    # sudouest. fr\n",
    "    text = re.sub('\\. fr(?!om)', '.fr', text)\n",
    "    # indepedent. ie\n",
    "    text = re.sub('\\. ie', '.ie', text)\n",
    "    # dbrs. dk\n",
    "    text = re.sub('\\. dk', '.dk', text)\n",
    "    # . per kilo\n",
    "    text = re.sub('\\.  per kilo', '.', text)\n",
    "    clean_texts.append(text)\n",
    "data['texts'] = clean_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f21c9c",
   "metadata": {},
   "source": [
    "Remove short texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29a2ae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.678262\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "# Save the result as a new column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a110f22",
   "metadata": {},
   "source": [
    "Articles with 20 or less words are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e204d407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6082\n"
     ]
    }
   ],
   "source": [
    "data = data[(data.word_count>20)]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77c9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data:\n",
    "drt_1 = os.path.dirname(path).replace('analysis', 'finance data')\n",
    "data.to_csv(os.path.join(drt_1,'articles_sorted.csv'), encoding = 'utf-8-sig', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FishText",
   "language": "python",
   "name": "fishtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
